---
title: Generative AI Possibilities and Adobe Firefly
date: 2023-05-24
category: Software
tags: machine learning
xposts:
  - label: Facebook
    url: https://www.facebook.com/harrislap/posts/pfbid0JL8HeQvhcPBN4TQgYL7WVxY3vCjNuzZkjeH58RvkUMsaoctjHFV8ACP4DwAM8Cfrl
---

I've critiqued conversations about new "generative AI models" for the past year as often having limited imagination about what these models can do. People tend to assume what they're being used to do *now* is what they will continue to be used to for in the future. Specifically "enter a prompt, get an image" for image models (like Midjourney and DALL-E) and "chatbots" for LLMs (like ChatGPT).

I think this has really limited conversation, because, for example, it's easy to see "enter a prompt, get an image" as a way to replace artists and discuss the technology on those grounds. But I've figured that these models aren't going to be used in that specific way primarily for long—they're more likely to become components in the artistic process, making certain things easier, lowering the barrier to entry to visual art, and making it trivial to accomplish certain visual effects that were previously inaccessible or challenging, much like modern cameras make certain types of photography trivial that were challenging a couple decades ago.

(I think something similar happens in conversations about ChatGPT. People see chatbots, but they don't see "cheap classifier," "image alt text writer," "document summarizer," or "style guide adherence checker." Some people see "code assistant"—partially because those products already exist—imo one of the most productive places we've found to deploy LLMs so far, already benefiting my work personally.)

Anyway, here's [the first tool](https://blog.adobe.com/en/publish/2023/05/23/future-of-photoshop-powered-by-adobe-firefly) I've seen based on the same general image generation technology as Stable Diffusion/DALL-E/Midjourney that's poised to go mainstream. It's from Adobe, baked into Photoshop, trained exclusively on image sets that Adobe has rights to, and to me feels like a natural successor to the content-aware fill functionality that visual artists have already used for a decade+ already. Very curious to see how it will be received.
